---
layout: distill
title: On Canonicalization in Geometric Deep Learning
date: 2025-05-25
description: Equivariant neural networks are powerful tools for machine learning tasks involving data with inherent symmetries. Their power stems from their ability to encode known transformations directly into their architecture, providing a strong inductive bias, especially in domains like physics, chemistry, and computer vision. This blog post delves into recent research addressing some of the key challenges that arise when applying these models.
tags: distill formatting

authors:
  - name: Changmin Kang
    # url: "https://sites.google.com/view/sungsooahn0215/home"
    affiliations:
      name: KAIST

bibliography: 2025-05-25-20244091.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: A quick cover on invariance and equivariance
  - name: Achieving in(equi)variance from existing models
  - name: Limitation of CAN
  - name: Other recent works
  - name: Conclusion

---

Invariant and equivariant neural networks have become a crucial tool in machine learning, particularly for tasks where data exhibits inherent *symmetries*. These symmetries, such as rotations or permutations, are mathematical transformations that leave certain properties of the data unchanged. By incorporating these symmetries into the network architecture, the enhanced models offer improved generalization, enhanced data efficiency, and greater interpretability. However, the development of such networks is not without its challenges.

We will delve into recent research that addresses some of the key challenges in the field of invariant/equivariant networks and how they are tackled. In particular, we will see how invariance/equivariance could be achieved for existing architectures via 'averaging', rather than by designing new architectures from the scratch. Then, we will see what are the shortcomings of the proposed 'averaging' methods and how they are recently revised.

## A quick cover on invariance and equivariance

In the field of geometric deep learning, the main focus is to design or obtain a network architecture that properly reflects the geometry, or the symmetries within the dataset. Such symmetries typically originate from the structure of the domain underlying the input signals. For instance, consider the image dataset (refer to the following figure by <d-cite key="bronstein2021geometric"></d-cite>). The domain $\Omega$ is a $n$-by-$n$ grid, where $n$ is an integer, and the space of signals $\mathcal{X}(\Omega)$ consists of the signals $x: \Omega \to \mathbb{R}^3$.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-05-25-20244091/DomSig.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

Assume that the machine learning system operates on signals on some domain $\Omega$, where the space of the signals $\mathcal{X}(\Omega)$ is a Hilbert space, that is, a linear combination and inner product among the signals are available.

<br>

Notice that the collection of symmetries satisfies a number of properties: a composition of two symmetries is again a symmetry, and its inverse, which always exists, is also a symmetry. These properties naturally define a group that consists of the symmetries. Formally, a *group* $G$ is a set equipped with a binary operation <d-footnote>For brevity and following the common notation, we juxtapose the first argument with the second. </d-footnote> $*:G\times G \to G$ satisfying the following axioms:

$$
\begin{gather*}
    \text{Associativity}: (g_1g_2)g_3 = g_1(g_2g_3),\quad \forall g_1, g_2, g_3 \in G.\\
    \text{Identity Element}: \exists e\in G \;\;\text{s.t.}\;\; eg = g = ge,\quad \forall g\in G.\\
    \text{Inverse Element}: \forall g\in G, \exists g^{-1}\in G \;\;\text{s.t.}\;\; gg^{-1} = e = g^{-1}g.
\end{gather*}
$$

<br>

The gist of the introduction of groups $(G,\*)$ of symmetry is to study how the group acts on the data. We will first define how the group acts on the domain $\Omega$. The action of the group on the signal space $\mathcal{X}(\Omega)$ would then be naturally obtained. A group $G$ acts on $\Omega$ by the (left) *group action* $\cdot : G\times\Omega \to \Omega$ that satisfies

$$
\begin{gather*}
    \text{Compatibility}: (g_1g_2)\cdot\omega = g_1\cdot(g_2\cdot \omega),\quad \forall g_1, g_2 \in G, \omega\in\Omega.\\
    \text{Identity Element}: e_G\cdot\omega = \omega,\quad \forall \omega\in\Omega.
\end{gather*}
$$

An action of $G$ on $\mathcal{X}(\Omega)$ is naturally obtained from that on $\Omega$:

$$
\begin{equation*}
    (g.x)(\omega) \triangleq x\left(g^{-1}\cdot\omega\right).
\end{equation*}
$$

As we assumed that the signal space is a Hilbert space, we can represent the action of each symmetry by some (invertible) matrix. Given some vector space $V$, a *group representation* is a map $\rho: G \to \mathrm{GL}(V)$ that assigns each group element to an invertible matrix and satisfies $\rho(gh) = \rho(g)\rho(h)$, for all $g, h \in G$.

<br>

Now we can define the invariance and the equivariance of a function. A function $f:\mathcal{X}(\Omega) \to \mathcal{Y}$ is $G$**-invariant** if, for all $g \in G, x\in\mathcal{X}$,

$$
f(\rho(g)x) = f(x).
$$

Similarly, a function $f:\mathcal{X}(\Omega) \to \mathcal{X}(\Omega')$ is $G$**-equivariant** if, for all $g \in G, x\in\mathcal{X}$,

$$
f(\rho(g)x) = \rho'(g)f(x),
$$

where $G$ is assumed to act both on $\mathcal{X}(\Omega)$ and $\mathcal{X}(\Omega')$, $\rho'$ is the group representation of $\mathcal{X}(\Omega')$. In words, a function is $G$-invariant if its output remains intact whatever the symmetry in $G$ acted on the input. On the other hand, a function is $G$-equivariant if its output is modified accordingly to the acted symmetry on the input.

---

## Achieving in(equi)variance from existing models

Given some model, or function $\phi:\mathcal{X} \to \mathcal{X}'$ and some symmetry group $G$ which acts both on the domain and the codomain, what are the ways to make them invariant/equivariant? The simplest way is to take the *average* of the function over the entire group, that is,

$$
\psi_\textrm{GA}(X) \triangleq \frac{1}{|G|}\sum_{g\in G}\rho_2(g)\phi(\rho_1(g)^{-1}X).
$$

This method is called **group averaging** (GA). It is a straightforward exercise to show that $\psi_\textrm{GA}$ is $G$-equivariant. Moreover, notice that omitting $\rho_2(g)$, or taking the group representation of the output space as the trival one, results in a $G$-invariant function for $\phi: \mathcal{X} \to \mathbb{R}$. Henceforth, we will unite the notation for invariance with that for equivariance for brevity (unless distinctions are required).

The critical problem of GA is that if the order of $G$ were extremely large (e.g., permutation group) or even infinite (e.g., Euclidean group) then the computation of GA would be intractable. As a solution, <d-cite key="puny2022frame"></d-cite> proposed **frame averaging** (FA): instead of averaging over entire group $G$, do it over a carefully selected subset of $G$, namely the frame $\mathcal{F}: \mathcal{X} \to 2^\mathcal{G}\setminus\varnothing$ (see below equation and figure).

$$
\lbrack \phi \rbrack_\mathcal{F}(X) \triangleq \frac{1}{|\mathcal{F}(X)|}\sum_{g\in \mathcal{F}(X)}\rho_2(g)\phi(\rho_1(g)^{-1}X).
\left( \right)
$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-05-25-20244091/FrameEqVar.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

<d-cite key="puny2022frame"></d-cite> proved that if $\mathcal{F}$ is a $G$-equivariant frame, or if $\mathcal{F}\left(\rho_1(g)X\right) = g\mathcal{F}(X) \coloneqq \{gh: h\in\mathcal{F}(X)\}$ holds, then $\lbrack \phi \rbrack_\mathcal{F}$ is $G$-equivariantâ€”if $\rho_2$ is trivial, then $\lbrack \phi \rbrack_\mathcal{F}$ is $G$-invariant (Theorem 1 of <d-cite key="puny2022frame"></d-cite>). Moreover, FA can provably impose additional symmetry (Theorem 2 of <d-cite key="puny2022frame"></d-cite>): if $\phi$ is $H$-equivariant(invariant) and $\mathcal{F}$ is $H$-invariant and $G$-equivariant, then $\lbrack \phi \rbrack_\mathcal{F}$ is $H\times G$-equivariant(invariant). <d-footnote> In addition, the group action by $G$ and $H$ should commute in input and output space. </d-footnote> FA also maintains the expressive power of backbone models (Theorem 4 of <d-cite key="puny2022frame"></d-cite>).

Despite of all these features, nothing can be presumed about the cardinality of the frame itself. It could be possible that the frame is smaller than the entire group $G$ but still be too large for computing the average. One example aligning with FA is the point cloud networks with Euclidean motions, $E_d$. Defining the frame based on the Principle Component Analysis (PCA), the size of the frame is $2^d$, which amounts to 8 when $d=3$.

---

## Limitation of CAN

asdf

---

## Other recent works

hjkl

---

## Conclusion

qwerty
