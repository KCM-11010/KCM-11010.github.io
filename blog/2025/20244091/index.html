<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> AI810 Blog Post (20244091) On Canonicalization in Geometric Deep Learning | Changmin Kang </title> <meta name="author" content="Changmin Kang"> <meta name="description" content="Equivariant neural networks are powerful tools for machine learning tasks involving data with inherent symmetries. Their power stems from their ability to encode known transformations directly into their architecture, providing a strong inductive bias, especially in domains like physics, chemistry, and computer vision. This blog post delves into recent research addressing some of the key challenges that arise when applying these models."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kcm-11010.github.io/blog/2025/20244091/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "AI810 Blog Post (20244091) On Canonicalization in Geometric Deep Learning",
            "description": "Equivariant neural networks are powerful tools for machine learning tasks involving data with inherent symmetries. Their power stems from their ability to encode known transformations directly into their architecture, providing a strong inductive bias, especially in domains like physics, chemistry, and computer vision. This blog post delves into recent research addressing some of the key challenges that arise when applying these models.",
            "published": "May 25, 2025",
            "authors": [
              
              {
                "author": "Changmin Kang",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "KAIST",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Changmin</span> Kang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>AI810 Blog Post (20244091) On Canonicalization in Geometric Deep Learning</h1> <p>Equivariant neural networks are powerful tools for machine learning tasks involving data with inherent symmetries. Their power stems from their ability to encode known transformations directly into their architecture, providing a strong inductive bias, especially in domains like physics, chemistry, and computer vision. This blog post delves into recent research addressing some of the key challenges that arise when applying these models.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#a-quick-cover-on-invariance-and-equivariance">A quick cover on invariance and equivariance</a> </div> <div> <a href="#frame-averaging-achieving-in-equivariance-from-existing-models">Frame Averaging-Achieving in/equivariance from existing models</a> </div> <div> <a href="#canonicalization-average-over-a-single-element">Canonicalization-Average over a single element</a> </div> <div> <a href="#weighted-frames-limitation-of-cn-in-continuity">Weighted Frames-Limitation of CN in continuity</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <p>Invariant and equivariant neural networks have become a crucial tool in machine learning, particularly for tasks where data exhibits inherent <em>symmetries</em>. These symmetries, such as rotations or permutations, are mathematical transformations that leave certain properties of the data unchanged. By incorporating these symmetries into the network architecture, the enhanced models offer improved generalization, enhanced data efficiency, and greater interpretability. However, the development of such networks is not without its challenges.</p> <p>We will delve into recent research that addresses some of the key challenges in the field of invariant/equivariant networks and how they are tackled. In particular, we will see how invariance/equivariance could be achieved for existing architectures via ‘averaging’, rather than by designing new architectures from the scratch. Then, we will see what are the shortcomings of the proposed ‘averaging’ methods and how they are recently revised.</p> <h2 id="a-quick-cover-on-invariance-and-equivariance">A quick cover on invariance and equivariance</h2> <p>In the field of geometric deep learning, the main focus is to design or obtain a network architecture that properly reflects the geometry, or the symmetries within the dataset. Such symmetries typically originate from the structure of the domain underlying the input signals. For instance, consider the image dataset (refer to the following figure by <d-cite key="bronstein2021geometric"></d-cite>). The domain $\Omega$ is a $n$-by-$n$ grid, where $n$ is an integer, and the space of signals $\mathcal{X}(\Omega)$ consists of the signals $x: \Omega \to \mathbb{R}^3$.</p> <div class="row mt-3"> <div class="col-sm-3 mt-3 mt-md-0"> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-05-25-20244091/DomSig-480.webp 480w,/assets/img/2025-05-25-20244091/DomSig-800.webp 800w,/assets/img/2025-05-25-20244091/DomSig-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2025-05-25-20244091/DomSig.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-3 mt-3 mt-md-0"> </div> </div> <p>Assume that the machine learning system operates on signals on some domain $\Omega$, where the space of the signals $\mathcal{X}(\Omega)$ is a Hilbert space, that is, a linear combination and inner product among the signals are available.</p> <p><br></p> <p>Notice that the collection of symmetries satisfies a number of properties: a composition of two symmetries is again a symmetry, and its inverse, which always exists, is also a symmetry. These properties naturally define a group that consists of the symmetries. Formally, a <em>group</em> $G$ is a set equipped with a binary operation <d-footnote>For brevity and following the common notation, we juxtapose the first argument with the second. </d-footnote> $*:G\times G \to G$ satisfying the following axioms:</p> \[\begin{gather*} \text{Associativity}: (g_1g_2)g_3 = g_1(g_2g_3),\quad \forall g_1, g_2, g_3 \in G.\\ \text{Identity Element}: \exists e\in G \;\;\text{s.t.}\;\; eg = g = ge,\quad \forall g\in G.\\ \text{Inverse Element}: \forall g\in G, \exists g^{-1}\in G \;\;\text{s.t.}\;\; gg^{-1} = e = g^{-1}g. \end{gather*}\] <p>For later use, we define the <em>orbit</em> of $x\in\mathcal{X}$, denoted as $[x] \triangleq \lbrace \rho(g)x:g\in G\rbrace$. The collection of all orbits are denoted as $\mathcal{X}/G$. We also define the <em>stabilizer</em> of $x$, denoted as $G_x \triangleq \lbrace g\in G: \rho(g)x = x \rbrace$.</p> <p><br></p> <p>The gist of the introduction of groups $(G,*)$ of symmetry is to study how the group acts on the data. We will first define how the group acts on the domain $\Omega$. The action of the group on the signal space $\mathcal{X}(\Omega)$ would then be naturally obtained. A group $G$ acts on $\Omega$ by the (left) <em>group action</em> $\cdot : G\times\Omega \to \Omega$ that satisfies</p> \[\begin{gather*} \text{Compatibility}: (g_1g_2)\cdot\omega = g_1\cdot(g_2\cdot \omega),\quad \forall g_1, g_2 \in G, \omega\in\Omega.\\ \text{Identity Element}: e_G\cdot\omega = \omega,\quad \forall \omega\in\Omega. \end{gather*}\] <p>An action of $G$ on $\mathcal{X}(\Omega)$ is naturally obtained from that on $\Omega$:</p> \[\begin{equation*} (g.x)(\omega) \triangleq x\left(g^{-1}\cdot\omega\right). \end{equation*}\] <p>As we assumed that the signal space is a Hilbert space, we can represent the action of each symmetry by some (invertible) matrix. Given some vector space $V$, a <em>group representation</em> is a map $\rho: G \to \mathrm{GL}(V)$ that assigns each group element to an invertible matrix and satisfies $\rho(gh) = \rho(g)\rho(h)$, for all $g, h \in G$.</p> <p><br></p> <p>Now we can define the invariance and the equivariance of a function. A function $f:\mathcal{X}(\Omega) \to \mathcal{Y}$ is $G$<strong>-invariant</strong> if, for all $g \in G, x\in\mathcal{X}$,</p> \[f(\rho(g)x) = f(x).\] <p>Similarly, a function $f:\mathcal{X}(\Omega) \to \mathcal{X}(\Omega’)$ is $G$<strong>-equivariant</strong> if, for all $g \in G, x\in\mathcal{X}$,</p> \[f(\rho_1(g)x) = \rho_2(g)f(x),\] <p>where $G$ is assumed to act both on $\mathcal{X}(\Omega)$ and $\mathcal{X}(\Omega’)$, $\rho_1$ and $\rho_2$ are the group representation of $\mathcal{X}(\Omega)$ and $\mathcal{X}(\Omega’)$, respectively. In words, a function is $G$-invariant if its output remains intact whatever the symmetry in $G$ acted on the input. On the other hand, a function is $G$-equivariant if its output is modified accordingly to the acted symmetry on the input.</p> <hr> <h2 id="frame-averaging-achieving-inequivariance-from-existing-models">Frame Averaging-Achieving in/equivariance from existing models</h2> <p>Given some model, or function $\phi:\mathcal{X} \to \mathcal{X}’$ and some group of symmetries $G$ which acts both on the domain and the codomain, what are the ways to make them in/equivariant? The simplest way is to take the <em>average</em> of the function over the entire group, that is,</p> \[\psi_\textrm{GA}(X) \triangleq \frac{1}{|G|}\sum_{g\in G}\rho_2(g)\phi(\rho_1(g)^{-1}X).\] <p>This method is called <strong>group averaging</strong> (GA). It is a straightforward exercise to show that $\psi_\textrm{GA}$ is $G$-equivariant. Moreover, notice that omitting $\rho_2(g)$, or taking the group representation of the output space as the trival one, results in a $G$-invariant function for $\phi: \mathcal{X} \to \mathbb{R}$. Henceforth, we shall omit formulas for invariance for brevity (unless distinctions are required).</p> <p>The critical problem of GA is that if the order of $G$ were extremely large (e.g., permutation group) or even infinite (e.g., Euclidean group) then the computation of GA would be intractable. As a solution, <d-cite key="puny2022frame"></d-cite> proposed <strong>frame averaging</strong> (FA): instead of averaging over entire group $G$, do it over a carefully selected subset of $G$, namely the frame $\mathcal{F}: \mathcal{X} \to 2^\mathcal{G}\setminus\varnothing$ (see below equation and figure).</p> \[\lbrack \phi \rbrack_\mathcal{F}(X) \triangleq \frac{1}{|\mathcal{F}(X)|}\sum_{g\in \mathcal{F}(X)}\rho_2(g)\phi(\rho_1(g)^{-1}X).\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-05-25-20244091/FrameEqVar-480.webp 480w,/assets/img/2025-05-25-20244091/FrameEqVar-800.webp 800w,/assets/img/2025-05-25-20244091/FrameEqVar-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2025-05-25-20244091/FrameEqVar.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>It is shown that if $\mathcal{F}$ is a $G$-equivariant frame, or if $\mathcal{F}\left(\rho_1(g)X\right) = g\mathcal{F}(X) = \lbrace gh: h\in\mathcal{F}(X)\rbrace$ holds, then $\lbrack \phi \rbrack_\mathcal{F}$ is $G$-equivariant—if $\rho_2$ is trivial, then $\lbrack \phi \rbrack_\mathcal{F}$ is $G$-invariant (Theorem 1 of <d-cite key="puny2022frame"></d-cite>). Moreover, FA can provably impose additional symmetry (Theorem 2 of <d-cite key="puny2022frame"></d-cite>): if $\phi$ is $H$-equivariant(invariant) and $\mathcal{F}$ is $H$-invariant and $G$-equivariant, then $\lbrack \phi \rbrack_\mathcal{F}$ is $H\times G$-equivariant(invariant). <d-footnote> In addition, the group action by $G$ and $H$ should commute in input (and output) space. </d-footnote> FA also maintains the expressive power of backbone models if the frame is bounded in terms of the operator norm (Theorem 4 of <d-cite key="puny2022frame"></d-cite>).</p> <p>Despite of all these features, nothing can be presumed about the cardinality of the frame itself. One example aligning with FA is the Euclidean motions $E_d$ acting on point cloud networks. Defining the frame based on the Principle Component Analysis (PCA), the size of the frame is $2^d$, which amounts to 8 when $d=3$. However, it could be possible that the frame is smaller than the entire group $G$ but still be too large for computing the average. To achieve equivariance in permutation groups $S_n$ acting on graph dataset with $n$ vertices, it is shown that $\vert\mathcal{F}(X)\vert \geq \vert\mathrm{Aut}(S_n)\vert = \vert S_n\vert$ that scales in order $n!$, so FA is inapplicable. <d-footnote> There is an efficient approximation if invariance were to be achieved by random sampling under uniform distribution. Interested readers may refer to Theorem 3 and Corollary 1 of <d-cite key="puny2022frame"></d-cite>. </d-footnote></p> <p>Below we show one experimental result of FA. The task is the normal estimation, or to estimate normal data from an input point cloud $\mathbb{R}^{n\times 3}$. The main backbone model is PointNet <d-cite key="qi2017pointnet"></d-cite> and DGCNN <d-cite key="wang2019dynamic"></d-cite>, which are popular and universal permutation equivariant 3D point cloud architectures. The experiment ran on three settings: i) $I/I$ - training and testing on the original data; ii) $I/SO(3)$ - training on the original data and testing on randomly rotated data; and iii) $SO(3)/SO(3)$ - training and testing with randomly rotated data. The ABC dataset <d-cite key="koch2019abc"></d-cite> that contains 3 collections (10k, 50k, and 100k models each) of Computer-Aided Design (CAD) models is employed for the experiment.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-05-25-20244091/FA_pointcloud_table-480.webp 480w,/assets/img/2025-05-25-20244091/FA_pointcloud_table-800.webp 800w,/assets/img/2025-05-25-20244091/FA_pointcloud_table-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2025-05-25-20244091/FA_pointcloud_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The quality of the normal estimation is $1-(n^\top\hat{n})^2$, with $n$ being the ground truth normal and $\hat{n}$ the normal prediction. FA incorporates the $E(3)$ symmetry as previously mentioned, resulting FA-PointNet/DGCNN models listed, which are also universal $E(3)\times S_n$ equivariant model. Another baseline of comparsion is vector neurons (VN-) <d-cite key="deng2021vector"></d-cite>, a framework for creating $SO(3)$-equivariant neural networks for pointcloud processing. Incorporating FA to existing architectures is beneficial in scenarios (ii-iii), outperforming augmentation by a large margin. In contrast to VN models, FA models maintain, or even improve the baseline estimation quality (i), attributed to the expressive power of the FA models.</p> <hr> <h2 id="canonicalization-average-over-a-single-element">Canonicalization-Average over a single element</h2> <p>FA can shrink the burden of GA but could still require numerous function calls. Instead of FA, one may consider canonicalization <d-cite key="kaba2023equivariance"></d-cite> to achieve in/equivariance by a single pass, or ‘averaging’ over a single symmetry.</p> <p>Let us first try to understand the intuition of canonicalization (CN) through the lens of a simple example: recognizing a handwritten digit ‘7’ regardless of its rotation. A standard non-equivariant classifier would need to learn to recognize ‘7’ in every possible orientation, which is inefficient and requires extensive data augmentation (viewpoint-independent). An equivariant model, on the other hand, is designed such that if it recognizes a ‘7’ in one orientation, it can easily recognize it in any other orientation (multiple-view). CN (single-view-plus-transformation) approach offers a different path: if we can find a consistent way to transform any rotated ‘7’ into a standard, upright ‘7’, then a simple, another model trained only on upright ‘7’s would suffice (and, if needed, rotate ‘7’ in the output space accordingly).</p> <div class="row mt-3"> <div class="col-sm-3 mt-2 mt-md-0"> </div> <div class="col-sm-6 mt-8 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-05-25-20244091/CN_view-480.webp 480w,/assets/img/2025-05-25-20244091/CN_view-800.webp 800w,/assets/img/2025-05-25-20244091/CN_view-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2025-05-25-20244091/CN_view.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-3 mt-2 mt-md-0"> </div> </div> <p>The central idea of the proposed framework is to decompose a model into two main components: a prediction function $f:\mathcal{X} \to \mathcal{X}’$ and a canonicalization (CN) function $h:\mathcal{X} \to \rho_1(G)$. CN function is designed to take an input $x$ and output an element of the transformation group $\rho(g)$ that, when applied to $x$, transforms it into a canonical representation. The overall model $\phi$ in canonicalized form is formulated as</p> \[\phi(x) \triangleq h'(x)f\left( h(x)^{-1} x \right),\] <p>where $h’=\rho_2\left( \rho_1^{-1}(h(x)) \right)$ is the counterpart of $h$ on the output. <d-footnote> As long as $\rho$ is faithful, one can make $\rho$ be bijective by a restriction of its codomain. </d-footnote> It is straightforward to show that $\phi$ is $G$-equivariant for <em>any</em> $f$ if $h$ is $G$-equivariant. <d-footnote> For invariance, simply omit $h'(x)$. </d-footnote> Hence, if $f$ can do the heavy-lifting part, $h$ need not to be an expressive, complex model as long as it is equivariant. This condition is further relaxed if $G$ has a normal subgroup $K$. Formally (Theorem 3.2 of <d-cite key="kaba2023equivariance"></d-cite>) for $G \simeq J\ltimes K $, if $f$ is $K$-equivariant and $h$ is $J$-equivariant, $K$-invariant, and its image is $\rho_1(J)$ then $\phi$ is $G$-equivariant. This theorem is highly relevant for practical applications, such as those involving the Euclidean group $E(d)$. The translation group $T(d)$ is a normal subgroup of $E(d)$, and $E(d)$ is a semidirect product of $T(d)$ and the orthogonal group. If the prediction network is designed to be translation-equivariant (as is the case with standard CNNs), Theorem 3.2 implies that CN function only needs to find the canonical rotation/reflection and be invariant to translations, rather than being able to output an element of the Euclidean group transforming equivariantly under rotations of the input.</p> <p>One can moreover claim the expressivity of CN, as FA did. If $f$ is a universal approximator of $K$-equivariant functions and $h$ (and $h’$) are continuous, the model $\phi$ in its CN form is a universal approximator of $G$-equivariant functions (Theorem 3.3 of <d-cite key="kaba2023equivariance"></d-cite>). This statement shall be revisited later.</p> <p><br></p> <p>The next step of CN is to obtain such $h$ encompassing equivariance. The first approach is to directly choose any existing equivariant neural network architecture with the output being a group element. For permutation groups and Lie groups (which includes Euclidean group), an equivariant multilayer perceptron <d-cite key="finzi2021practical"></d-cite> could be selected. Another approach is to find take $h$ as the minimizer of some energy function $E$. Formally,</p> \[h(x) \in \arg\min E\left(\rho_1(g)^{-1}x\right),\] <p>where ties in symmetries are randomly broken. Intuitively, $E$ represents a distance between the input and the canonical sample of the orbit and is therefore minimized when $\rho_1(g)$ is the transformation that maps to the canonical sample.<d-footnote> Frankly speaking, $h(x)$ is a minimizer of a scoring function that satisfies some equivariance-like condition between $g$ and $x$ and its minimum value should be equal in each orbit. The formulation $E\left(\rho_1(g)^{-1}x\right)$ naturally meets the first condition, but the second condition is not formally proved. According to the authors, however, the second condition can be met almost surely for neural networks under weak assumption <d-cite key="cox2020almost"></d-cite>. Curious readers may look at Section 4 and Appendix B of <d-cite key="kaba2023equivariance"></d-cite>. </d-footnote> For instance, in image classification, the images are transformed into point clouds then fed to Deep Sets <d-cite key="zaheer2017deep"></d-cite> to be returned as the energy. The mapping $E$ is optimized by gradient descent (using implicit differentiation).</p> <p>We introduce $N$-body dynamics experiment regarding CN. <d-footnote> This experiment is also shown in <d-cite key="puny2022frame"></d-cite>, but not demonstrated in the post. Rather than explaining each and every compared models in the table below, besides FA-GNN, we ask readers to refer to <d-cite key="puny2022frame"></d-cite>. </d-footnote> In this task, the model has to predict the future positions of $N=5$ charged particles interacting with Coulomb force given initial positions and velocities <d-cite key="satorras2021n"></d-cite>.</p> <div class="row mt-3"> <div class="col-sm-4 mt-3 mt-md-0"> </div> <div class="col-sm-4 mt-6 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-05-25-20244091/CN_Nbody_table-480.webp 480w,/assets/img/2025-05-25-20244091/CN_Nbody_table-800.webp 800w,/assets/img/2025-05-25-20244091/CN_Nbody_table-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2025-05-25-20244091/CN_Nbody_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> </div> </div> <p>The metric reported is the Mean Squared Error between predicted locations and ground truth. The prediction function $f$ is a 4-layer GNN <d-footnote> with same hyperparameters as <d-cite key="puny2022frame"></d-cite>. </d-footnote>. The CN function is a simple 2-layer Vector Neurons version <d-cite key="deng2021vector"></d-cite> of the Deep Sets, which has 20 times fewer parameters than $f$ has. Despite $h$ being simple, CN-GNN, trained end-to-end, improves the performance compared to the baseline and FA-GNN. There are two variants of CN-GNN. The first one is CN-GNN-$O(3)$, where CN is only learned for the $O(3)$ part of the transformation; the translation part is given by the centroid. As choosing the center of mass as the origin of the reference frame is natural, the performance is expected to be and is, in fact, marginally degraded. The second variant is CN-GNN (frozen), where $h$ is not learned. Although the model is still equivariant with respect to Euclidean group, it shows smaller loss than the baseline.</p> <p><br></p> <p>Though CN seems practical, in the sense that it requires one function call, it has its weaknesses. One limitation is that the actual equivariance may not be achieved for all inputs. If, for any $x\in\mathcal{X}$ and any $g\in G, g\ne e$ such that $\rho(g)x = x$, $h(\rho(g)x) = h(x) \neq \rho(g)h(x)$ in general. In practice, such cases are extremely rare, or belong to a measure-zero set—think about how ‘many’ images would be invariant under any single symmetry in Euclidean group except identity symmetry. This would not be problematic in practice, but this limitation makes the paper that proposes a new method less sound. The authors defined <em>relaxed equivariance</em> to address the problem. For $f:\mathcal{X}\to\mathcal{X}’$ and their group representation $\rho$ and $\rho’$, respectively, $f$ satisfies relaxed equivariance if</p> \[\forall g_1, x, \; \exists g_2\in g_1 G_x\quad\text{such that}\quad f(\rho(g_1)x) = \rho'(g_2)f(x).\] <p>where $G_x$ is the stabilizer of $x$, $G_x$. <d-footnote> so, the problematic inputs for standard CN are those with non-trivial stabilizers. </d-footnote> Moreover, the model $\phi$ in its canonicalized form satisfies the relaxed equivariance condition if CN function $h$ satisfies the relaxed equivariance condition. Whether how much the relaxed equivariance aligns with the standard equivariance in other literature should be discussed.</p> <p>Another limitation is even more critical—the existence of continuous CN function. Recall that, for the universality result, the continuity of $h$ and $h’$ were necessary. This assumption might not always hold, as the existence of a continuous equivariant function depends on the specific group action and the topology of the input and output spaces. Consequently, the applicability of the framework could be limited in scenarios where such continuous canonicalization functions do not exist. Let us dive deeper into this perspective.</p> <hr> <h2 id="weighted-frames-limitation-of-cn-in-continuity">Weighted Frames-Limitation of CN in continuity</h2> <p>The answer for the guarantee in continuous equivariant function via CN is given by <d-cite key="dym2024equivariant"></d-cite>. Surprisingly, a continuous CN may not exist for rotation and symmetric groups even if the model $\phi$ itself is continuous. How does this counter-intuitive argument hold?</p> <p>Let us see how. <d-footnote> The authors assumes compactness of group $G$ and the continuity of group action $G$ on domain (and codomain). Readers may refer to Appendix A of <d-cite key="dym2024equivariant"></d-cite> for more details. </d-footnote> Let $F(\mathcal{X}, \mathcal{X}’)$ denote the space of functions from $\mathcal{X}$ to $\mathcal{X}’$, and let $C(\mathcal{X}, \mathcal{X}’)$ denote the subset of these functions which are also continuous. We say a linear operator $\mathcal{E}: F(\mathcal{X}, \mathcal{X}’)\to F(\mathcal{X}, \mathcal{X}’)$ is a</p> \[\begin{align*} \text{Bounded operator}: \forall \text{ compact } K, \exists C_K &gt; 0 \;\;\text{s.t.}\;\; \max_{x\in K} \|\mathcal{E}(f)(x)\| \leq C_K \max_{x\in K} \|f(x)\|.\\ \text{Equivariant projection operator}: \forall f\in F(\mathcal{X},\mathcal{X}'), \mathcal{E}(f) \text{ is equivariant.}\\\quad\text{ If } f \text{ is already equivariant, then } \mathcal{E}(f) = f.\\ \text{Continuity-Preserving}: f\in C(\mathcal{X}, \mathcal{X}') \Rightarrow \mathcal{E}(f)\in C(\mathcal{X}, \mathcal{X}'). \end{align*}\] <d-footnote> The last condition is re-stated to align with the acronym BEC. In the original text, the condition is 'Preserves continuity'. </d-footnote> <p>If all conditions are met, we say $\mathcal{E}$ is a BEC operator. One nice property of being BEC operator is that preserves universality and continuity of a collection of models while adding equivariance (Proposition 2.2 of <d-cite key="dym2024equivariant"></d-cite>). <d-footnote> From now on, we denote the operator as $\mathcal{I}$ if it is invariant projection operator instead of being equivaraint. </d-footnote> Recall that FA also maintains universality if the frame is bounded; but the main challenge of this paper is to also preserve continuity. We also inform the readers that GA is already a bounded, continuity-preserving, and equivariant(invariant) projection, but as we discussed, GA is highly inefficient in terms of the order of the group.</p> <p>Before moving onto the impossibility of continuous CN, we first define the orbit canonicalization and how this terminology relates to the original CN. Intuitively, an orbit canonicalization $y : \mathcal{X} → \mathcal{X}$ maps all elements in any given orbit to a unique orbit element (and, “averages” over it.) Formally, such $y$ is an orbit canonicalization if</p> \[\begin{gather*} \forall g\in G, \forall x \in\mathcal{X}, y(g\cdot x) = y(x).\\ \forall x \in\mathcal{X}, y(x) \in [x]. \end{gather*}\] <p>If the action of $G$ on $\mathcal{X}$ is free, or if $G_x = \lbrace e \rbrace$ for all $x$, then it naturally induces a CN $h:\mathcal{X}\to G$, in the sense that $y(x) = h(x)^{-1}\cdot x$, and such formulation is unique. For instance, consider a translation group of $\mathbb{R}^d$ on a point cloud $X\in\mathbb{R}^{d\times n}, X = (x_1, \ldots, x_n)$. A simple orbit canonicalization is $(x_1, x_2, \ldots, x_n) \mapsto (0, x_2-x_1, \ldots, x_n-x_1)$ and the associated CN is $h(X) = x_1$. Moreover, $\mathcal{I}<em>{CN}(f)(x) := f(h(x)^{-1}x)$ and $\mathcal{E}</em>{CN}(f)(x) := h(x)f(h(x)^{-1}x)$ are invariant and equivariant projection operators and bounded. In addition, both operations preserve continuity if and only if $y$ is continuous (Proposition 2.5 of <d-cite key="dym2024equivariant"></d-cite>; it is an exercise for readers to come up with an example for $\mathcal{E}_can$). Below are an illustration of continuous/discontinuous orbit canonicalizations.</p> <div class="row mt-3"> <div class="col-sm mt-6 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-05-25-20244091/WFA_CNdiscont-480.webp 480w,/assets/img/2025-05-25-20244091/WFA_CNdiscont-800.webp 800w,/assets/img/2025-05-25-20244091/WFA_CNdiscont-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2025-05-25-20244091/WFA_CNdiscont.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The first orbit canonicalization, on $\mathbb{R}^n$ with respect to $S_n$ via sorting is continuous. However, that on $\mathbb{R}^{2\times n}$ via lexicographic sorting is not. Another discontinuous case is to rotate a point cloud in $\mathbb{R}^{2\times n}$ so that the first point lies on positive part of x-axis. </div> <p>One can actually prove that the action of $O(d), SO(d)$ on $\mathbb{R}^{d\times n}$ admits no continuous canonicalization if $n &gt; d$ (Proposition 2.7 of <d-cite key="dym2024equivariant"></d-cite>). <d-footnote> This result is tight, in the sense that if $n \leq d$, then there exists continuous canonicalization. </d-footnote> Typically, modern dataset presumes $n &gt; d$, hence no continuous CN can exist.</p> <p>Interestingly, the impossibility of continuity for CN extends to FA. The critical result is Theorem 2.8: If the set of points in domain with trivial stabilizer is a connected set, then for each points in the closure of that set, the size of the frame is the order of the group to preserve continuity. This implies that for permutation group $S_n$ acting on $\mathbb{R}^{d\times n}$, the continuity-preserving frame is exactly $S_n$ (Corollary 2.9 of <d-cite key="dym2024equivariant"></d-cite>). In addition, the continuity-preserving frame for $SO(d)$ on $\mathbb{R}^{2\times n}$ with $n \geq 2$ cannot be finite (Theorem 2.10 of <d-cite key="dym2024equivariant"></d-cite>).</p> <p><br></p> <p>Now that we have seen the impossibility part, the immediate question should ask about remedies of this issue. A generalization of FA, <em>weighted frame</em> by <d-cite key="dym2024equivariant"></d-cite> is one solution. Although the formal definition requires knowledge in probability measure theory, the interest of <d-cite key="dym2024equivariant"></d-cite> is on the measures with finite support. That is, we consider the weighted average of the model outputs over the selected symmetries. A weighted frame is equivariant if both the group elements and their assigned weights are accordingly modified. It is <em>weakly equivariant</em> if the average over the stabilizer (at some point) is equivariant. <d-footnote> Formally, a weight frame $\mu_{\cdot}$ is a mapping from a group element to a Borel probability measure on (topological) $G$. It is equivariant at $x$ if for all $g$, $\mu_{gx}(A) = \mu_x(g^{-1}A)$ where $A\subset G$. For $x\in\mathcal{X}$ and a probability measure $\tau$, definte $⟨\tau⟩_x(A) = \int_{G_x} \tau(g^{-1}A)dg$. $\mu_{\cdot}$ is said to be weakly equivariant if $\bar{\mu}_{gx}(A) = \bar{\mu}_x(g^{-1}A)$, where $\bar{\mu}_x$ stands for $⟨\mu_x⟩_x$. It is straightforward to see that equivariant implies weakly equivariant, and these two are equivalent if $G_x$ is trivial. The advantage of weakly equivariant is that the support of resulting probability measure need not to be at least the same size of the stabilizer, which holds for equivariant case.</d-footnote> Both equivariant and weakly equivariant weighted frames can be used to define invariant and equivariant projection operators, just as we saw in FA. Moreover, if the corresponding operators are continuous (we say the weighted frame is <em>robust</em> if it is continuous and weakly equivariant), the operators now preserve continuity and are bounded, as desired. <d-footnote> In fact, a weaker notation of continuity (in terms of weak topology) in weighted frame is still sufficient to guarantee the continuity of the output of the operators. See Section 3.1 of <d-cite key="dym2024equivariant"></d-cite> for details. </d-footnote> <d-footnote> Frankly speaking, for the proposed equivariant projection operator to return a equivariant, continuous model requires additional constraint: stability. It requires the average computed by $\mu_x$ and $\bar{\mu}_x$ should agree, and the average computed by $\mu_{x_n}$ should coverge to that by $\mu_x$, for any sequence $x_n \to x$. Curious readers may refer to Section 6 and Appendix E for details and examples. </d-footnote> It is shown that a robust frame requires at most polynomial and finite symmetries in that weight average for permutation and orthogonal group, respectively, much smaller than what FA would require (we saw that the former needs factorial and the latter never suffices with any finite number; for actual quantity and their derivation, see Section 4 and 5).</p> <p>We close the discussion upon weighted frame with an experimental result. <d-cite key="dym2024equivariant"></d-cite> conducted one about the action of permutation group on two-dimensional point cloud. The image of each digit in MNIST dataset is processed into point cloud, and 4-layer MLP is employed for the classification. For weighted frame and GA (which corresponds to Reynolds Operator in the table below), due to the size of the dataset, their implementation underwent empirical averaging—one random sample in each train step and 1, 5, 10, 25 samples for inference. The whole configuration can be found in section 7.</p> <div class="row mt-3"> <div class="col-sm mt-6 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-05-25-20244091/WF_table-480.webp 480w,/assets/img/2025-05-25-20244091/WF_table-800.webp 800w,/assets/img/2025-05-25-20244091/WF_table-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2025-05-25-20244091/WF_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>There, “Robust Frames (Sec 4.1)” regards a subset of $\mathbb{R}^{2\times N}$ where the permutation group freely acts on; “Robust Frames (Sec 4.2)” deals with the whole space. Notice that no invariance and Reynolds Operator do not work well, since the permutation group is so large that both of these are essentially not enforcing any permutation invariance. Moreover, (discontinuous) CN works better than these two methods, but worse than robust frames as the learned model lacks continuity.</p> <p>One question that can be asked to the weighted frame method is ‘how discontinuous is the canonicalization function via vanilla CN?’ It is globally discontinuous, but could be so at a very few portion of the input space, or even at most countably many points. If latter is the case, it could explain why the performace of vanilla CN is slightly worse than that of weighted frame. As there is a trade-off between CN and weighted frame in terms of performance and computational cost, this would be an interesting future direction in this line. <d-footnote> There is an additional experiment to show the discontinuity of vanilla CN at 'bad' points, or points whose stabilizer is infinite, conducted on 3D point cloud data on ModelNet40 dataset. Interested readers are referred to Appendix F. </d-footnote></p> <hr> <h2 id="conclusion">Conclusion</h2> <p>In this blog post, we have seen a series of methods that are able to inject in/equivariance of a given group to existing architectures. The three take-home messages to keep in mind are the following:</p> <ul> <li>The simplest way is to average the function output over (carefully selected) symmetry elements.</li> <li>Canonicalization (CN) may achieve to do so with single function call, but whether it truly is possible by CN and the expressivity of CN models are yet to be guaranteed.</li> <li>Using weighted frames, both the continuity and the in/equivariance of the model are satisfied.</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-05-25-20244091.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Changmin Kang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>